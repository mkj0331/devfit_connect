from backend_single_analysis_method import collect_spring_backend_files, filter_backend_files_by_keywords, summarize_file_with_llm, split_into_batches,summarize_batch_semantic, analyze_project_from_batches, analyze_commit_style, call_with_retry
import os
import json
from dotenv import load_dotenv
import time

load_dotenv()

gms_api_key=os.getenv("GMS_API_KEY")
gms_base_url=os.getenv("GMS_BASE_URL")


OWNER = 'HTTP501'
REPO = 'idk'

position = 'backend' # backend / frontend

if position == 'backend':
    backend_framework = 'spring' # spring / fastapi / django

    if backend_framework == "spring":
        language = "Java"    
        TARGET_DIRS = {"controller", "service", "repository"}
        SERVICE_KEYWORDS = ["@Transactional","if (", "for (", "while (","try {", "catch (","throw new","validate", "check","Event", "publish"]
        REPOSITORY_KEYWORDS = ["@Query","nativeQuery", "join","fetch","existsBy","findBy","countBy"]
    elif backend_framework == "fastapi":
        language = "Python"
        TARGET_DIRS = {"routers", "router", "api", "services", "service", "crud", "db", "models", "schemas"}
        SERVICE_KEYWORDS = ["Depends(", "def ", "async def", "if ", "for ", "while ", "try:", "except", "raise ", "validate", "process", "logic"]
        REPOSITORY_KEYWORDS = ["select(", "insert(", "update(", "delete(", "join(", "where(", "session.execute", "db.query", "await session", "commit()"]
    elif backend_framework == "django":
        language = "Python"
        TARGET_DIRS = {"views", "models", "serializers", "services", "repositories"}
        SERVICE_KEYWORDS = ["def ", "class ", "if ", "for ", "try:", "except", "raise ", "validate", "process"]
        REPOSITORY_KEYWORDS = [".objects.filter", ".objects.get", ".objects.create", ".objects.update", ".objects.exclude", ".objects.annotate", ".objects.aggregate", "select_related", "prefetch_related"]

user_input = "ë°±ì—”ë“œì—ì„œ ëˆí¬ì¼“, ëª©í‘œì €ì¶•, ìë™ì´ì²´ ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤"

repo_root = r"C:\Users\SSAFY\Desktop\S14P11B111\ë¬¸ê²½ì§„\github_crawl\idk"


#################################################
### í•„ìš”í•œ íŒŒì¼ ì„ ë³„
selected_files = collect_spring_backend_files(repo_root, TARGET_DIRS=TARGET_DIRS)
print("1ì°¨ ì„ ë³„:", len(selected_files)) # í•µì‹¬ ë””ë ‰í† ë¦¬ë¡œ ì„ ë³„

filtered_files = filter_backend_files_by_keywords(selected_files, SERVICE_KEYWORDS=SERVICE_KEYWORDS, REPOSITORY_KEYWORDS=REPOSITORY_KEYWORDS)
print("2ì°¨ ì„ ë³„:", len(filtered_files)) # ì½”ë“œ ë‚´ í•µì‹¬ keywordë¡œ ì„ ë³„


#####################################################
### ê°œë³„ íŒŒì¼ ë¶„ì„ ë° ìš”ì•½
BASE_SLEEP = 1.5          # ê¸°ë³¸ ëŒ€ê¸°
LONG_SLEEP_EVERY = 15     # 15ê°œë§ˆë‹¤
LONG_SLEEP_TIME = 180     # 3ë¶„

filtered_file_summaries = []

for i, file in enumerate(filtered_files):
    result = call_with_retry(
        lambda: summarize_file_with_llm(
            path=file["path"],
            content=file["content"],
            gms_api_key=gms_api_key,
            gms_base_url=gms_base_url,
            user_input = user_input
        )
    )

    filtered_file_summaries.append(result)

    INDIVIDUAL_DIR = r"C:\Users\SSAFY\Desktop\S14P11B111\ë¬¸ê²½ì§„\backend_single_analysis\individual_summaries"
    os.makedirs(INDIVIDUAL_DIR, exist_ok=True)
    
    output_path = os.path.join(INDIVIDUAL_DIR, f"{REPO}_files_{i}.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ì„ ë³„ëœ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    time.sleep(BASE_SLEEP)

    # ğŸ›‘ ëˆ„ì  ì¿¼í„° íšŒí”¼ìš© ê°•ì œ íœ´ì‹
    if (i + 1) % LONG_SLEEP_EVERY == 0:
        print(f"â¸ï¸ {LONG_SLEEP_EVERY}ê°œ ì²˜ë¦¬ â†’ {LONG_SLEEP_TIME//60}ë¶„ íœ´ì‹")
        time.sleep(LONG_SLEEP_TIME)

        
###############################
### ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
batches = split_into_batches(
    items=filtered_file_summaries,
    batch_size=10
)
print("ë°°ì¹˜(10)ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì™„ë£Œ")


##########################################################

# 5. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ë¯¸ ìš”ì•½(LLM 1íšŒ x batch ìˆ˜)
batch_semantic_summaries = []
BATCH_SUMMARY_DIR = r"C:\Users\SSAFY\Desktop\S14P11B111\ë¬¸ê²½ì§„\backend_single_analysis\batch_summaries"
os.makedirs(BATCH_SUMMARY_DIR, exist_ok=True)

total = len(batches)

for idx, batch in enumerate(batches, 1):
    print(f"[BATCH {idx}/{total}] START")

    batch_json = {
        "batch_id": idx,
        "files_count": len(batch),
        "summaries": batch
    }

    try:
        semantic = summarize_batch_semantic(
            batch_data=batch_json,
            gms_api_key=gms_api_key,
            gms_base_url=gms_base_url
        )

        batch_semantic_summaries.append(semantic)

        output_path = os.path.join(
            BATCH_SUMMARY_DIR,
            f"batch_{idx}_semantic.json"
        )

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(semantic, f, ensure_ascii=False, indent=2)

        print(f"[BATCH {idx}/{total}] DONE")

    except Exception as e:
        print(f"[BATCH {idx}/{total}] ERROR :: {e}")

print("ë°°ì¹˜ ë³„ ìš”ì•½ ì™„ë£Œ")


# ==============================
# 6. ìš”ì•½ëœ íŒŒì¼ë“¤ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
# ==============================
final_result = analyze_project_from_batches(
    batch_semantic_summaries=batch_semantic_summaries,
    gms_api_key=gms_api_key,
    gms_base_url=gms_base_url, 
    repo_analysis_id=f"{OWNER}_{REPO}"
)

final_result['language'] = language # ì–¸ì–´ ìš°ì„  Java ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„



# ==============================
# 7. ì»¤ë°‹ ìŠ¤íƒ€ì¼ ë¶„ì„í•´ì„œ í˜‘ì—… ì§€í‘œ ë¦¬í¬íŠ¸ì— ì¶”ê°€
# =============================

with open(
    rf"C:\Users\SSAFY\Desktop\S14P11B111\ë¬¸ê²½ì§„\github_crawl\{REPO}\{OWNER}_{REPO}_commit_metadata.json",
    encoding="utf-8"
) as f:
    commit_metadata = json.load(f)

commit_style = analyze_commit_style(
    commit_metadata=commit_metadata,
    gms_api_key=gms_api_key,
    gms_base_url=gms_base_url
)
final_result["collaboration_style"] = commit_style


# ==============================
# 9. ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥
# ==============================
BASE_OUTPUT_DIR = r"C:\Users\SSAFY\Desktop\S14P11B111\ë¬¸ê²½ì§„\backend_single_analysis"

final_output_path = os.path.join(
    BASE_OUTPUT_DIR, f"{OWNER}_{REPO}_single_analysis.json"
)

with open(final_output_path, "w", encoding="utf-8") as f:
    json.dump(final_result, f, ensure_ascii=False, indent=2)

print("í”„ë¡œì íŠ¸ ìµœì¢… ë¶„ì„ ì™„ë£Œ")